# -*- coding: utf-8 -*-
"""Основы ИИ лаб 4

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Q8z4s37V2kZcqqLuPnvii_E7F3WxMkHb
"""

# Commented out IPython magic to ensure Python compatibility.
import numpy as np
import tensorflow as tf
from tensorflow import keras
import pandas as pd
import seaborn as sns
from pylab import rcParams
import string
import matplotlib.pyplot as plt
import math
from matplotlib import rc
from google.colab import drive
from sklearn.model_selection import train_test_split
from collections import Counter, defaultdict
from sklearn.metrics import accuracy_score
from sklearn.metrics import classification_report, confusion_matrix
import struct
from scipy.special import expit
from sklearn.manifold import TSNE
from ast import literal_eval

# %matplotlib inline

sns.set(style='whitegrid', palette='muted', font_scale=1.5)

rcParams['figure.figsize'] = 14, 8

RANDOM_SEED = 42

np.random.seed(RANDOM_SEED)

(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()

def mle(y, axis=1):
    return np.argmax(y, axis)


def one_hot(y, n_labels):
    mat = np.zeros((len(y), n_labels))
    for i, val in enumerate(y):
        mat[i, val] = 1
    return mat

def L2_reg(lambda_, w1, w2):
    return (lambda_ / 2.0) * (np.sum(w1 ** 2) + np.sum(w2 ** 2))


def L1_reg(lambda_, w1, w2):
    return (lambda_ / 2.0) * (np.abs(w1).sum() + np.abs(w2).sum())

def cross_entropy(outputs, y_target):
    return -np.sum(np.log(outputs) * y_target, axis=1)

def plot_product(X, y, idx):
    img = X[idx].reshape(28,28)
    plt.imshow(img, cmap=plt.cm.binary)
    plt.title('true label: %s' % CLASS_NAMES[y[idx]])
    plt.colorbar()
    plt.grid(False)
    plt.show()

def plot_error(model):
    plt.plot(range(len(model.error_)), model.error_)
    plt.ylabel('Error')
    plt.xlabel('Epochs')
    plt.show()

def plot_image(i, predictions_array, true_labels, images):
  predictions_array, true_label, img = predictions_array[i], true_labels[i], images[i]
  plt.grid(False)
  plt.xticks([])
  plt.yticks([])

  plt.imshow(img.reshape(28,28), cmap=plt.cm.binary)

  predicted_label = np.argmax(predictions_array)
  if predicted_label == true_label:
    color = 'blue'
  else:
    color = 'red'

  plt.xlabel("{} {:2.0f}% ({})".format(CLASS_NAMES[predicted_label],
                                100*np.max(predictions_array),
                                CLASS_NAMES[true_label]),
                                color=color)

def plot_value_array(i, predictions_array, true_label):
  predictions_array, true_label = predictions_array[i], true_label[i]
  plt.grid(False)
  plt.xticks([])
  plt.yticks([])
  thisplot = plt.bar(range(10), predictions_array, color="#777777")
  plt.ylim([0, 1])
  predicted_label = np.argmax(predictions_array)

  thisplot[predicted_label].set_color('red')
  thisplot[true_label].set_color('blue')

def plot_image_grid(X_test, y_test, y_hat, num_rows=5, num_cols=3):
  num_images = num_rows*num_cols
  plt.figure(figsize=(2*2*num_cols, 2*num_rows))
  for i in range(num_images):
    plt.subplot(num_rows, 2*num_cols, 2*i+1)
    plot_image(i, y_hat, y_test, X_test)
    plt.subplot(num_rows, 2*num_cols, 2*i+2)
    plot_value_array(i, y_hat,  y_test)

"""# Image data

## Reading images
"""

(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()

"""## Exploration"""

CLASS_NAMES = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]
plot_product(x_train, y_train, idx=0)

x_train.shape
#.reshape(28,28)

plot_product(x_train, y_train, idx=2)

plot_product(x_train, y_train, idx=3)

"""
# Background

## Sigmoid (and it's derivative)

$$\sigma (x) = \frac{1}{1+e^{-x}}$$

"""

def sigmoid(z):
    # return 1.0 / (1.0 + np.exp(-z))
    return expit(z)

"""### First derivative of Sigmoid

$$\frac{d\sigma (x)}{d(x)} = \sigma (x)\cdot (1-\sigma(x))$$
"""

def sigmoid_prime(z):
    sg = sigmoid(z)
    return sg * (1 - sg)

"""## Softmax


$$\sigma(z)_j = \frac{e^{z_j}}{\sum_{K}^{k=1}e^{z_k}} \text{for}\,j = 1,...,k$$
"""

def softmax(z):
    return (np.exp(z.T) / np.sum(np.exp(z), axis=1)).T

"""# Neural Network Image Classifier

## Model implementation
"""

class NNClassifier:

    def __init__(self, n_classes, n_features, n_hidden_units=30,
                 l1=0.0, l2=0.0, epochs=500, learning_rate=0.01,
                 n_batches=1, random_seed=None):

        if random_seed:
            np.random.seed(random_seed)
        self.n_classes = n_classes
        self.n_features = n_features
        self.n_hidden_units = n_hidden_units
        self.w1, self.w2 = self._init_weights()
        self.l1 = l1
        self.l2 = l2
        self.epochs = epochs
        self.learning_rate = learning_rate
        self.n_batches = n_batches

    def _init_weights(self):
        w1 = np.random.uniform(-1.0, 1.0,
                               size=(self.n_hidden_units, self.n_features))
        w2 = np.random.uniform(-1.0, 1.0,
                               size=(self.n_classes, self.n_hidden_units))
        return w1, w2

    def _forward(self, X):
        net_input = X.copy()
        net_hidden = self.w1.dot(net_input.T)
        act_hidden = sigmoid(net_hidden)
        net_out = self.w2.dot(act_hidden)
        act_out = sigmoid(net_out)
        return net_input, net_hidden, act_hidden, net_out, act_out

    def _backward(self, net_input, net_hidden, act_hidden, act_out, y):
        sigma3 = act_out - y
        sigma2 = self.w2.T.dot(sigma3) * sigmoid_prime(net_hidden)
        grad1 = sigma2.dot(net_input)
        grad2 = sigma3.dot(act_hidden.T)
        return grad1, grad2

    def _error(self, y, output):
        L1_term = L1_reg(self.l1, self.w1, self.w2)
        L2_term = L2_reg(self.l2, self.w1, self.w2)
        error = cross_entropy(output, y) + L1_term + L2_term #return -np.sum(np.log(outputs) * y_target, axis=1)
        return 0.5 * np.mean(error)

    def L2_reg(lambda_, w1, w2):
        return (lambda_ / 2.0) * (np.sum(w1 ** 2) + np.sum(w2 ** 2))


    def L1_reg(lambda_, w1, w2):
        return (lambda_ / 2.0) * (np.abs(w1).sum() + np.abs(w2).sum())

    def _backprop_step(self, X, y):
        net_input, net_hidden, act_hidden, net_out, act_out = self._forward(X)
        y = y.T

        grad1, grad2 = self._backward(net_input, net_hidden, act_hidden, act_out, y)

        # regularize
        grad1 += (self.w1 * (self.l1 + self.l2))
        grad2 += (self.w2 * (self.l1 + self.l2))

        error = self._error(y, act_out)

        return error, grad1, grad2

    def predict(self, X):
        Xt = X.copy()
        net_input, net_hidden, act_hidden, net_out, act_out = self._forward(Xt)
        return mle(net_out.T)

    def predict_proba(self, X):
        Xt = X.copy()
        net_input, net_hidden, act_hidden, net_out, act_out = self._forward(Xt)
        return softmax(act_out.T)

    def fit(self, X, y):
        self.error_ = []
        X_data, y_data = X.copy(), y.copy()
        y_data_enc = one_hot(y_data, self.n_classes)

        X_mbs = np.array_split(X_data, self.n_batches)
        y_mbs = np.array_split(y_data_enc, self.n_batches)

        for i in range(self.epochs):

            epoch_errors = []

            for Xi, yi in zip(X_mbs, y_mbs):

                # update weights
                error, grad1, grad2 = self._backprop_step(Xi, yi)
                epoch_errors.append(error)
                self.w1 -= (self.learning_rate * grad1)
                self.w2 -= (self.learning_rate * grad2)
            self.error_.append(np.mean(epoch_errors))
        return self

    def score(self, X, y):
        y_hat = self.predict(X)
        return np.sum(y == y_hat, axis=0) / float(X.shape[0])

nn.w2, nn.w1

nn.w1.T.dot(-1)

"""## Training"""

y_train = np.delete(y_train, np.arange(1000,60000),0)
y_train.shape

x_train = np.delete(x_train, np.arange(1000,60000),0)
x_train.shape

nn = NNClassifier(
    n_classes=10,
    n_features=784,
    n_hidden_units=8,
    l2=0.0,
    l1=0.0,
    epochs=100,
    learning_rate=0.001,
    n_batches=25,
    random_seed=RANDOM_SEED
).fit(x_train.reshape(1000, 784), y_train);

nn.tmp1

"""## Evaluation"""

plot_error(nn)

x_test.shape

print('Train Accuracy: %.2f%%' % (nn.score(x_train.reshape(1000, 784), y_train) * 100))
print('Test Accuracy: %.2f%%' % (nn.score(x_test.reshape(10000, 784), y_test) * 100))

nn.predict(x_train.reshape(1000,784))